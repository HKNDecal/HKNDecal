{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pooch\n",
    "%pip install textx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import Audio\n",
    "from os.path import getsize\n",
    "from scipy.datasets import face\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions \n",
    "\n",
    "This is an Jupyter notebook. Notebooks makes it easy to run small snippets in\n",
    "  Python code to see what happens! Here are some useful commands to use:\n",
    "   \n",
    "  1. Click on a cell (will have a green outline around it) and press\n",
    "      <kbd>Shift</kbd> + <kbd>Enter</kbd> to run the code in the cell. Make sure you run the top cell\n",
    "      first, because it contains important import statements!\n",
    "      \n",
    "  2. You can also run a cell, or all the cells, using the 'Cell' menu up top.\n",
    "  \n",
    "  3. Save your work with <kbd>Ctrl</kbd> + <kbd>S</kbd>, or in the File menu.\n",
    "  \n",
    "  If you have any questions, just ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Images as matrices of numbers\n",
    "\n",
    "For the following section, create some pixel art in greyscale. That means\n",
    "using numbers from 0-1, 0 being totally black and 1 being totally white.\n",
    "\n",
    "Don't spend tooooo much time on this, but make sure your picture has some\n",
    "different shades of grey. Press <kbd>Shift</kbd> + <kbd>Enter</kbd> while hovering on this cell to\n",
    "update your changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to modify the numbers below to change the colors\n",
    "my_image = np.array([\n",
    "    [1, 0.75, 0.5, 0.25, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.2],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.2, 0.3],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "    [0.0, 0.0, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    [0.0, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "])\n",
    "plt.imshow(my_image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Image Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I wanted to make put a watermark or a YouTube annotation on the image? I have to overlay my image on top of your image. Let's say that my annotation, which is 9 pixels, looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = np.array([\n",
    "    [0.5, 0.9, 0.5],\n",
    "    [0.9, 0.5, 0.9],\n",
    "    [0.5, 0.9, 0.5]\n",
    "])\n",
    "plt.imshow(annotation, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overlaying the images takes a couple steps:\n",
    "\n",
    "1. Create an image that is totally blank except for my annotation, exactly where I want it. \n",
    "2. Create a mask with 1s and 0s that correspond to the part of the image that takes up the annotation. Multiplying this mask with the image will remove that part of the original image (an AND operation).\n",
    "3. \"Add\" the two images together (OR operation). The final result will have your image with my annotation in the corner!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create annotation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Here is an image with all zeros except for my annotation:\")\n",
    "\n",
    "annotation_in_corner = np.array([\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.9, 0.5],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.9, 0.5, 0.9],\n",
    "    [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.9, 0.5],\n",
    "])\n",
    "plt.imshow(annotation_in_corner, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Create Bitmask and multiply (AND) by the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "\"\"\"\n",
    "This bitmask will keep all of the pixels of your image except for the bottom 9 in the corner when I multiply it with your image.\n",
    "In effect, the pixel is nonzero if my pixel AND yours are nonzero.\n",
    "\"\"\")\n",
    "\n",
    "bitmask = np.array([\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0],\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0],\n",
    "    [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0]\n",
    "])\n",
    "plt.imshow(bitmask, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Now, let's multiply my bitmask and your image together! This is what the array looks like:\")\n",
    "\n",
    "my_image_masked = my_image * bitmask\n",
    "print(my_image_masked)\n",
    "plt.imshow(my_image_masked, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Finally, let's create the end image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = my_image_masked + annotation_in_corner\n",
    "\n",
    "plt.imshow(result, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Image Transformations\n",
    "\n",
    "Now that we have an image, we can make modifications to it by changing each pixel's value in a systematic way. \n",
    "\n",
    "We can brighten, darken, change the white balance, or change the color of the image with simple lines of code!\n",
    "\n",
    "Let's apply some transformations to your image! Try to guess what each code block is doing before you run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.0 The original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image = result\n",
    "print(my_image)\n",
    "plt.imshow(result, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 What would IPython show?!?!\n",
    "\n",
    "This is the first transformation. Run the code block when you're ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image2 = np.round(my_image)\n",
    "    \n",
    "print(\".ot resolc saw ti eno hcihw no desab ,etihw ro kcalb rehtie ot lexip hcae strevnoc noitamrofsnart\\\n",
    " siht rof edoc ehT\"[::-1])\n",
    "print(my_image2)\n",
    "plt.imshow(my_image2, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 What would IPython show?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter = -0.5\n",
    "my_image3 = np.clip(my_image + my_filter, 0, 1)\n",
    "\n",
    "print(\"Here is what the matrix looks like:\")\n",
    "print(my_image3)\n",
    "print(\"?neppah lliw kniht uoy od tahW .evitisop ti gnikam neve ro elbairav retlif_ym eht gnignahc\\\n",
    " yrT !ti ot tnuoma (evitagen() tnatsnoc a sdda dna rebmun hcae sekat tI .retlif gninekrad a si eno sihT\"[::-1])\n",
    "\n",
    "plt.imshow(my_image3, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Filtering with full-sized images\n",
    "\n",
    "Now let's work with a real image. This image has way more pixels than your pixel art, but the\n",
    "structure is exactly the same! (note: not *exactly* the same, because the numbers are now from 0-255).\n",
    "\n",
    "Let's apply some of the same \"filters\" that we applied to your pixel art. We'll start with a greyscale version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raccoon_gray = face(gray=True)\n",
    "plt.imshow(raccoon_gray, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code to apply the black and white filter\n",
    "raccoon_gray_bw = raccoon_gray.copy()\n",
    "for row_index, row in enumerate(raccoon_gray_bw):\n",
    "    for pixel_index, pixel in enumerate(row):\n",
    "        raccoon_gray_bw[row_index][pixel_index] = 255 if pixel > 128 else 0\n",
    "        \n",
    "plt.imshow(raccoon_gray_bw, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a darkening (negative) or lightening (positive) amount here!\n",
    "# remember that the values range from 0-255 now.\n",
    "my_filter = -100\n",
    "\n",
    "# Run this code to apply the darkening and lightening filter.\n",
    "raccoon_gray_darken = np.clip(raccoon_gray + my_filter, 0, 255)\n",
    "\n",
    "\n",
    "plt.imshow(raccoon_gray_darken, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.4 Working with color images\n",
    "\n",
    "For our final step, let's work with color. Now, instead of there being one number for each pixel, there are three. These are the Red, Green, and Blue (RGB) values that you've probably heard of. \n",
    "\n",
    "We can manipulate these values in a similar way as with greyscale, but now we have even more creative control. Let's give our raccoon a nice Instagram filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, view the original picture in all its glory!\n",
    "raccoon = face()\n",
    "plt.imshow(raccoon)\n",
    "# Notice that each \"item\" of the data grid is now 3 values\n",
    "# corresponding to Red, Green, and Blue values between 0 and 255\n",
    "# The code below prints the top left corner, in a 3x3 region\n",
    "print(raccoon[:3, :3, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, it's time to modify the color values. We will boost some colors by increasing their magnitude.\n",
    "raccoon_r, raccoon_g, raccoon_b = np.dsplit(raccoon.astype(int), raccoon.shape[-1])\n",
    "\n",
    "# play around with these values if you want!\n",
    "lighten = -20\n",
    "red_boost = -10 + lighten\n",
    "green_boost = 45 + lighten\n",
    "blue_boost = 30 + lighten\n",
    "\n",
    "raccoon_filtered = np.clip(np.dstack([\n",
    "    raccoon_r + red_boost,\n",
    "    raccoon_g + green_boost,\n",
    "    raccoon_b + blue_boost\n",
    "]), 0, 255).astype(np.uint8)\n",
    "plt.imshow(raccoon_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Image Compression by Grouping Similar Pixels\n",
    "\n",
    "The raccoon image above takes $768 \\times  1024 \\times 3 = 2359296 \\text{ bytes} = 2.36 \\text{ MB}$ (each number is `uint8`, which is 1 byte).\n",
    "\n",
    "That's pretty big for one picture! To put that in perspective, a one minute long video running at 24 frames per second using the above encoding will have a size of $2.36 * 24 * 60 = 3398.4 \\text{ MB} = 3.4 \\text{ GB}$!\n",
    "\n",
    "One way to compress images is by reducing the amount of unique colors in the image. \n",
    "\n",
    "The code below groups an image's pixels by color into n clusters, and it replaces each pixel in the image with the center of the cluster that the pixel belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_im(im: np.ndarray, n_clusters: int, samples: int = 1000) -> np.ndarray:\n",
    "    '''Compresses color image by finding the top n_groups of pixels, \n",
    "    then replacing all pixels with their closest neighbor in that group.\n",
    "    \n",
    "    Don't worry about the specifics of this function.\n",
    "    '''\n",
    "    # house keeping code to massage the data formats\n",
    "    if im.dtype == np.uint8:\n",
    "        im = im.astype(np.float64) / 255        \n",
    "    shape = im.shape    \n",
    "    im_X = im.reshape(shape[0]*shape[1], 3)\n",
    "    im_X_to_sample = im_X.copy()\n",
    "    shuffle(im_X_to_sample)\n",
    "    im_X_sampled = im_X_to_sample[:samples]\n",
    "    \n",
    "    # using KMeans algorithm to find n clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10).fit(im_X_sampled)\n",
    "    \n",
    "    # reconstructing compressed image with the found clusters\n",
    "    centers = kmeans.cluster_centers_\n",
    "    group_assignments = kmeans.predict(im_X)\n",
    "    im_Y = np.take(centers, group_assignments, axis=0)\n",
    "    im_compressed = im_Y.reshape(shape[0], shape[1], 3)\n",
    "    return im_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original image has roughly {raccoon.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary the number of clusters and see how the results change!\n",
    "\n",
    "1. What happens when you just have 1 cluster? 2?\n",
    "2. How many clusters do you need to have more than \"1\" dimension of color?\n",
    "3. What's the minimum number of clusters so that you can't tell the difference between the original and the compressed image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "compressed_raccoon = compress_im(raccoon, n_clusters)\n",
    "plt.imshow(compressed_raccoon)\n",
    "print(f\"Compressed image has roughly {compressed_raccoon.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like images, sounds are also just arrays of numbers that describe the \"intensity\" of the sound at each step. \n",
    "\n",
    "Sound files need to specify a \"sample rate\" in the unit of hertz (1/second), which describes how frequent each \"intensity\" step is. \n",
    "\n",
    "For example a sound file with a sample rate of 10 means its numbers describe sound intensities that change every 0.1s.\n",
    "\n",
    "Standard quality audio often has a sample rate of 44100 = 44k. Meaning the sound intensity can very 44 thousand times in a second!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textx.metamodel import metamodel_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_table = {\n",
    "    'A#0': 29.14,\n",
    "    'A#1': 58.27,\n",
    "    'A#2': 116.54,\n",
    "    'A#3': 233.08,\n",
    "    'A#4': 466.16,\n",
    "    'A#5': 932.33,\n",
    "    'A#6': 1864.66,\n",
    "    'A#7': 3729.31,\n",
    "    'A#8': 7458.62,\n",
    "    'A0': 27.5,\n",
    "    'A1': 55.0,\n",
    "    'A2': 110.0,\n",
    "    'A3': 220.0,\n",
    "    'A4': 440.0,\n",
    "    'A5': 880.0,\n",
    "    'A6': 1760.0,\n",
    "    'A7': 3520.0,\n",
    "    'A8': 7040.0,\n",
    "    'Ab0': 25.96,\n",
    "    'Ab1': 51.91,\n",
    "    'Ab2': 103.83,\n",
    "    'Ab3': 207.65,\n",
    "    'Ab4': 415.3,\n",
    "    'Ab5': 830.61,\n",
    "    'Ab6': 1661.22,\n",
    "    'Ab7': 3322.44,\n",
    "    'Ab8': 6644.88,\n",
    "    'B0': 30.87,\n",
    "    'B1': 61.74,\n",
    "    'B2': 123.47,\n",
    "    'B3': 246.94,\n",
    "    'B4': 493.88,\n",
    "    'B5': 987.77,\n",
    "    'B6': 1975.53,\n",
    "    'B7': 3951.07,\n",
    "    'B8': 7902.13,\n",
    "    'Bb0': 29.14,\n",
    "    'Bb1': 58.27,\n",
    "    'Bb2': 116.54,\n",
    "    'Bb3': 233.08,\n",
    "    'Bb4': 466.16,\n",
    "    'Bb5': 932.33,\n",
    "    'Bb6': 1864.66,\n",
    "    'Bb7': 3729.31,\n",
    "    'Bb8': 7458.62,\n",
    "    'C#0': 17.32,\n",
    "    'C#1': 34.65,\n",
    "    'C#2': 69.3,\n",
    "    'C#3': 138.59,\n",
    "    'C#4': 277.18,\n",
    "    'C#5': 554.37,\n",
    "    'C#6': 1108.73,\n",
    "    'C#7': 2217.46,\n",
    "    'C#8': 4434.92,\n",
    "    'C0': 16.35,\n",
    "    'C1': 32.7,\n",
    "    'C2': 65.41,\n",
    "    'C3': 130.81,\n",
    "    'C4': 261.63,\n",
    "    'C5': 523.25,\n",
    "    'C6': 1046.5,\n",
    "    'C7': 2093.0,\n",
    "    'C8': 4186.01,\n",
    "    'D#0': 19.45,\n",
    "    'D#1': 38.89,\n",
    "    'D#2': 77.78,\n",
    "    'D#3': 155.56,\n",
    "    'D#4': 311.13,\n",
    "    'D#5': 622.25,\n",
    "    'D#6': 1244.51,\n",
    "    'D#7': 2489.02,\n",
    "    'D#8': 4978.03,\n",
    "    'D0': 18.35,\n",
    "    'D1': 36.71,\n",
    "    'D2': 73.42,\n",
    "    'D3': 146.83,\n",
    "    'D4': 293.66,\n",
    "    'D5': 587.33,\n",
    "    'D6': 1174.66,\n",
    "    'D7': 2349.32,\n",
    "    'D8': 4698.63,\n",
    "    'Db0': 17.32,\n",
    "    'Db1': 34.65,\n",
    "    'Db2': 69.3,\n",
    "    'Db3': 138.59,\n",
    "    'Db4': 277.18,\n",
    "    'Db5': 554.37,\n",
    "    'Db6': 1108.73,\n",
    "    'Db7': 2217.46,\n",
    "    'Db8': 4434.92,\n",
    "    'E0': 20.6,\n",
    "    'E1': 41.2,\n",
    "    'E2': 82.41,\n",
    "    'E3': 164.81,\n",
    "    'E4': 329.63,\n",
    "    'E5': 659.25,\n",
    "    'E6': 1318.51,\n",
    "    'E7': 2637.02,\n",
    "    'E8': 5274.04,\n",
    "    'Eb0': 19.45,\n",
    "    'Eb1': 38.89,\n",
    "    'Eb2': 77.78,\n",
    "    'Eb3': 155.56,\n",
    "    'Eb4': 311.13,\n",
    "    'Eb5': 622.25,\n",
    "    'Eb6': 1244.51,\n",
    "    'Eb7': 2489.02,\n",
    "    'Eb8': 4978.03,\n",
    "    'F#0': 23.12,\n",
    "    'F#1': 46.25,\n",
    "    'F#2': 92.5,\n",
    "    'F#3': 185.0,\n",
    "    'F#4': 369.99,\n",
    "    'F#5': 739.99,\n",
    "    'F#6': 1479.98,\n",
    "    'F#7': 2959.96,\n",
    "    'F#8': 5919.91,\n",
    "    'F0': 21.83,\n",
    "    'F1': 43.65,\n",
    "    'F2': 87.31,\n",
    "    'F3': 174.61,\n",
    "    'F4': 349.23,\n",
    "    'F5': 698.46,\n",
    "    'F6': 1396.91,\n",
    "    'F7': 2793.83,\n",
    "    'F8': 5587.65,\n",
    "    'G#0': 25.96,\n",
    "    'G#1': 51.91,\n",
    "    'G#2': 103.83,\n",
    "    'G#3': 207.65,\n",
    "    'G#4': 415.3,\n",
    "    'G#5': 830.61,\n",
    "    'G#6': 1661.22,\n",
    "    'G#7': 3322.44,\n",
    "    'G#8': 6644.88,\n",
    "    'G0': 24.5,\n",
    "    'G1': 49.0,\n",
    "    'G2': 98.0,\n",
    "    'G3': 196.0,\n",
    "    'G4': 392.0,\n",
    "    'G5': 783.99,\n",
    "    'G6': 1567.98,\n",
    "    'G7': 3135.96,\n",
    "    'G8': 6271.93,\n",
    "    'Gb0': 23.12,\n",
    "    'Gb1': 46.25,\n",
    "    'Gb2': 92.5,\n",
    "    'Gb3': 185.0,\n",
    "    'Gb4': 369.99,\n",
    "    'Gb5': 739.99,\n",
    "    'Gb6': 1479.98,\n",
    "    'Gb7': 2959.96,\n",
    "    'Gb8': 5919.91,\n",
    "    'N': 0 # nothing\n",
    "}\n",
    "\n",
    "music_metamodel = metamodel_from_file(\"music.tx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Sounds as numbers in time domain\n",
    "\n",
    "Instructions:\n",
    "1. Use the following code to visualize and play the \"A\" chord. \n",
    "2. Vary the sample rate to higher and lower numbers - can you hear the difference?\n",
    "3. Change the frequencies array to a sequence of other keys (see a mapping of keys to frequencies [here](https://pages.mtu.edu/~suits/notefreqs.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play around with these values!\n",
    "frequencies = [440, 554.37, 659.25]\n",
    "sample_rate = 44100\n",
    "\n",
    "# Constructing audio\n",
    "samples = np.array([np.sin(np.linspace(0, 2 * np.pi * frequency, sample_rate)) for frequency in frequencies])\n",
    "\n",
    "# Displaying plot\n",
    "data_rate = sample_rate\n",
    "data = np.sum(samples, axis=0)\n",
    "plt.plot(np.arange(data.shape[0]), data, alpha=0.3)\n",
    "plt.title(\"Time Domain Intensities\")\n",
    "plt.ylabel(\"Intensity\")\n",
    "plt.xlabel(\"Time step\")\n",
    "\n",
    "Audio(np.concatenate(list(samples) + [np.sum(samples, axis=0)]), rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the frequencies of the audio above, we will convert 0.1s \"chunks\" of the song into the frequency domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_chunks(data: np.ndarray, chunks_per_second: int = 1, datarate: int = 44100) -> np.ndarray:\n",
    "    num_chunks = int(round(chunks_per_second * len(data) / datarate))\n",
    "    chunks = [data[int(datarate / chunks_per_second * i) : int(datarate / chunks_per_second * (i + 1))] for i in range(num_chunks)]\n",
    "    frequencies = np.asarray([np.abs(np.fft.rfft(chunk)) for chunk in chunks]).T\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(20*np.log10(to_chunks(data, 1, sample_rate))[:1000], interpolation=\"none\", aspect=\"auto\", cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sounds as numbers in frequency domain\n",
    "\n",
    "\"Frequency\" encoding is very similar to the dictionary-based encoding scheme above. Instead of each key referring to a word, each key here refers to a \"chunk\" of sound played at the specified frequency of that key.\n",
    "\n",
    "1. Look at `lullaby.txt` for the frequency (keys) encoding of the lullaby.\n",
    "2. Run the following code to listen to the audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_song(filename):\n",
    "    return music_metamodel.model_from_file(filename)\n",
    "\n",
    "def song_to_frequencies(song):\n",
    "    parts = {}\n",
    "    for part in song.parts:\n",
    "        parts[part.name] = [\n",
    "            (frequency_table[note.note], note.duration) for note in part.notes\n",
    "        ]\n",
    "    return parts\n",
    "\n",
    "def frequencies_to_stream(freq, samplerate, bpm):\n",
    "    # 1 beat / beats per minute * min per second * samples per second\n",
    "    data = []\n",
    "    for f, beats in freq:\n",
    "        seconds = beats / bpm * 60\n",
    "        data.append(np.cos(np.linspace(0, 2 * np.pi * f * seconds, int(round(samplerate * seconds)))))\n",
    "    return np.concatenate(data)\n",
    "\n",
    "def smooth(data, length):\n",
    "    return np.convolve(np.ones(length) / length, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 256\n",
    "sampling_rate = 44100*16\n",
    "\n",
    "# Change this to change the number of beats per minute\n",
    "beats_per_minute = 100\n",
    "\n",
    "song = load_song(\"lullaby.txt\") # change the file name to whatever you created\n",
    "frequencies = song_to_frequencies(song)\n",
    "\n",
    "# Replace the names with the part names you chose\n",
    "song_parts = [\"melody\", \"harmony\", \"left_hand_high\", \"left_hand_low\"]\n",
    "song_data = sum(frequencies_to_stream(frequencies[name], sampling_rate, beats_per_minute) for name in song_parts)\n",
    "Audio(smooth(song_data, 256)[::sampling_rate // 44100], rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to visualize \"chunks\" of the lullaby in the frequency domain\n",
    "\n",
    "What does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_music = np.log10(to_chunks(song_data, 1, data_rate) + 0.001)[:120][::-1]\n",
    "plt.imshow(sheet_music, interpolation=\"none\", aspect=\"auto\", cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compression\n",
    "Below is the compression ratio for the song. Notice that if we store the file as text (as a list of frequencies) rather than raw audio data (which is a list of bytes over time), we can reduce the size of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lullaby_size = getsize(\"lullaby.txt\")\n",
    "\n",
    "print(f\"Sheet music size: {song_data.nbytes} bytes\")\n",
    "print(f\"Audio data size: {lullaby_size} bytes\")\n",
    "print(f\"Total compression ratio: {song_data.nbytes / lullaby_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try modifying the file `lullaby.txt` with your own notes or writing your own file!\n",
    "\n",
    "Notes are written in standard notation (i.e. Bb3) means the third B-flat. Rests are written as N.\n",
    "\n",
    "Ab3 4 means play the third A-flat for 4 beats.\n",
    "\n",
    "Each part must have the same number of beats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit lullaby.txt like:\n",
    "```\n",
    "[part_name_1]\n",
    "notes\n",
    "\n",
    "[part_name_2]\n",
    "notes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the previous two sections of the notebook again, replacing `melody`, `harmony` and the other names with your part names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## 2.3 Compression in JPEG (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to audio, there can also be frequencies found in 2D images. It turns out, we can describe any 8x8 image as the coefficients of 64 different \"basis images\", each with different frequencies horizontally and vertically. \n",
    "\n",
    "Below is a look at how analyzing these 2d frequencies are used in practice, specifically for JPEG compression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is what some of those 2d frequencies look like.\n",
    "# play around with the x and y frequencies by \n",
    "# toggling x_factor and y_factor!\n",
    "\n",
    "x_factor = 7 # should be between 0 and 8.\n",
    "y_factor = 7 # should be between 0 and 8.\n",
    "base_freq = np.pi / 16.0\n",
    "\n",
    "# creates the (i, j) basis image of the discrete cosine transform (DCT) used in JPEG\n",
    "def basis_image(x_fact: int, y_fact: int) -> np.ndarray:\n",
    "    image = np.array([\n",
    "        [np.cos(base_freq * x_fact * (2*j + 1)) * np.cos(base_freq * y_fact * (2*i + 1)) for j in range(8)] \n",
    "        for i in range(8)\n",
    "    ])\n",
    "\n",
    "    image = 0.5 * (image + 1) # scaling / shifting so every value is between 0 and 1\n",
    "    return image\n",
    "\n",
    "plt.imshow(basis_image(x_factor, y_factor), cmap=\"gray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we linearly combine two or more of these basis images, we can get really complex patterns!\n",
    "\n",
    "This is useful in compression, as we can combine these images to make a lot of the same patterns you would see in an actual image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_1 = basis_image(4, 1)\n",
    "basis_2 = basis_image(5, 5)\n",
    "\n",
    "alpha = 0.3 # what fraction of the first image to add\n",
    "\n",
    "plt.imshow(alpha * basis_1 + (1 - alpha) * basis_2, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JPEG works by breaking up every 8x8 pixel chunk of an image, and calculating the 64 coefficients of these basis images that would all add together to recreate it.\n",
    "\n",
    "You may be wondering where the compression in this is-- the original chunk had 64 pixels, so wouldn't storing the 64 coefficients not be any better?\n",
    "\n",
    "This is exactly right. Where JPEG gets its compression is by throwing away coefficients it doesn't need. For example, if an image doesn't recquire high-detailed texture, one can throw away the high frequency images and still have a fairly recognizable overall image. Also, if some of the coefficients are very small, it may be better not to add them to the final JPEG at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.fftpack as fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREDIT: Frank Ong for EE 123:\n",
    "# https://inst.eecs.berkeley.edu/~ee123/sp16/Sections/JPEG_DCT_Demo.html\n",
    "\n",
    "# Without going into too much detail, this code determines the 64 \n",
    "# coefficients for each 8x8 chunk, then removes the ones that are not larger\n",
    "# than some threshold. Once the low coefficients are gone, it recreates the \n",
    "# image by adding back all those scaled basis images we calculated earlier.\n",
    "\n",
    "def dct2(a: np.ndarray) -> np.ndarray:\n",
    "    return fft.dct(fft.dct(a, axis=0, norm='ortho'), axis=1, norm='ortho')\n",
    "\n",
    "def idct2(a: np.ndarray) -> np.ndarray:\n",
    "    return fft.idct(fft.idct(a, axis=0 , norm='ortho'), axis=1 , norm='ortho')\n",
    "\n",
    "def jpeg_compress_img(im: np.ndarray, threshold: float) -> np.ndarray:\n",
    "    im_w, im_l = im.shape # dimensions of the image\n",
    "    im_dct = np.zeros(im.shape) # creating an empty image to store the frequency coefficients\n",
    "    \n",
    "    # Do 8x8 DCT on image (in-place), using fancy numpy indexing magic!\n",
    "    for i in np.arange(0, im_w, 8):\n",
    "        for j in np.arange(0, im_l, 8):\n",
    "            im_dct[i:i+8, j:j+8] = dct2(im[i:i+8, j:j+8])\n",
    "            \n",
    "    # Applying threshold to each block:\n",
    "    \n",
    "    dct_thresh = im_dct * (np.abs(im_dct) > (threshold * np.amax(im_dct)))\n",
    "    \n",
    "    percent_nonzeros = float(np.sum(dct_thresh != 0.0) / (im_w * im_l))\n",
    "\n",
    "    print(f\"Keeping only {percent_nonzeros:.2%} of the DCT coefficients\")\n",
    "    \n",
    "    # getting back the compressed image\n",
    "    \n",
    "    im_idct = np.zeros(im.shape)\n",
    "    for i in np.arange(0, im_w, 8):\n",
    "        for j in np.arange(0, im_l, 8):\n",
    "            im_idct[i:i+8, j:j+8] = idct2(dct_thresh[i:i+8, j:j+8] )\n",
    "            \n",
    "    # normalize:\n",
    "    im_idct = im_idct / np.amax(im_idct)\n",
    "    \n",
    "    return im_idct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_raccoon = raccoon_gray.copy()\n",
    "plt.imshow(original_raccoon, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.3 # increase to decrease quality.\n",
    "compressed_raccoon = jpeg_compress_img(original_raccoon, thresh)\n",
    "plt.imshow(compressed_raccoon, cmap=\"gray\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
