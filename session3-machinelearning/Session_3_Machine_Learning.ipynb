{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "#### Going Down the EECS Stack DeCal Fall 2023\n",
    "\n",
    "In this notebook, we'll explore various machine learning techniques and look at a few datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll explore some ideas behind machine learning using housing data from San Francisco and New York. </br>\n",
    "Data courtesy of [r2d3](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "\n",
    "<img src=\"files/sf_ny_comparison.jpg\" alt=\"image from https://grapecollective.com/media/article/image/cache/720x337-center/c/o/comparison.jpg\">\n",
    "\n",
    "Each data point corresponds to a house with the following fields:\n",
    "- year_built (int, e.g. 1990)\n",
    "- price_per_sqft (int, dollars)\n",
    "- bath (float)\n",
    "- beds (float)\n",
    "- elevation (int, ft)\n",
    "- price (int, dollars)\n",
    "- in_sf (int, 1 if in sf, 0 otherwise)\n",
    "\n",
    "In the first part of this notebook we'll try to classify houses and predict whether a house is from NYC or SF. In the second part we'll try to regress and predict the prices of the houses.\n",
    "\n",
    "<br />\n",
    "Mathy Notation for later parts:\n",
    "- $n$ denotes the number of data points (houses) \n",
    "- $d$ denotes the number of features \n",
    "- $X$ is an $n \\times d$ matrix, where each row corresponds to a house. $X_i$ means the $i$th row, or the $i$th feature vector.\n",
    "- $y$ is a length $n$ vector, where each index corresponds to a label. $y_i$ means the label for the $i$th house. For part 1 the labels are either $1$ or $0$ for SF and NYC respectively. For part 2 the labels are real numbers denoting housing price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to install required packages.\n",
    "!pip3 install numpy \"scikit-learn==0.24.1\" matplotlib \"scipy==1.6.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from util3 import extract_cols, visualize_linear_regression, visualize_perceptron, load_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load housing data\n",
    "all_data, features_c, labels_c, features_r, labels_r = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Classification: NYC vs SF\n",
    "In this section, our goal is to learn a model that predicts whether a given house is from SF or from NYC. Our label is \"is_sf\", which is 1 if the house belongs to SF, 0 if NYC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 Manual Classification\n",
    "1. Use the cells below to explore statistics about the housing data. Note the mean and range of certain features. Which feature helps differentiating nyc and sf houses the most?\n",
    "2. Use your knowledge about the data to fill in the function \"is_in_sf,\" which takes in a feature dictionary and returns 1 if you think this house belongs in SF, 0 if it belongs to NY\n",
    "3. Test how good your result is. Try to improve your score as much as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_housing_histogram(feature_name):\n",
    "    '''\n",
    "    statistic is either 'mean', 'std', 'min', 'max'\n",
    "    feel free to modify this function to explore other properties about the housing data.\n",
    "    '''\n",
    "    if feature_name not in features_c[0]:\n",
    "        raise ValueError(\"Invalid feature_name!\")\n",
    "        \n",
    "    sf = []\n",
    "    nyc = []\n",
    "    for data in all_data:\n",
    "        if data['in_sf']:\n",
    "            sf.append(data[feature_name])\n",
    "        else:\n",
    "            nyc.append(data[feature_name])\n",
    "    \n",
    "    plt.figure()\n",
    "    bins = np.histogram(np.hstack((sf, nyc)), bins=50)[1]\n",
    "    plt.hist(sf, bins, alpha=0.5, facecolor='red', label='SF')\n",
    "    plt.hist(nyc, bins, alpha=0.5, facecolor='blue', label='NYC')\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('number of houses')\n",
    "    plt.title('Histogram of SF and NYC Houses by {}'.format(feature_name))\n",
    "    plt.legend()\n",
    "    \n",
    "    print('SF | mean: {}, std: {}, min: {}, max: {}'.format(np.mean(sf), np.std(sf), min(sf), max(sf)))\n",
    "    print('NYC | mean: {}, std: {}, min: {}, max: {}'.format(np.mean(nyc), np.std(nyc), min(nyc), max(nyc)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) explore the data\n",
    "# available features are price, year_built, bath, beds, elevation, price_per_sqft\n",
    "plot_housing_histogram('price')\n",
    "plot_housing_histogram('year_built')\n",
    "plot_housing_histogram('bath')\n",
    "plot_housing_histogram('beds')\n",
    "plot_housing_histogram('elevation')\n",
    "plot_housing_histogram('price_per_sqft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Implement is_in_sf\n",
    "def is_in_sf(feature):\n",
    "    '''\n",
    "    feature is a dictionary with the following keys: \n",
    "    - 'year_built'\n",
    "    - 'price_per_sqft'\n",
    "    - 'bath'\n",
    "    - 'beds'\n",
    "    - 'elevation'\n",
    "    - 'price'\n",
    "    \n",
    "    feature[key] returns the values of the feature for each house\n",
    "    Example: feature['price'] >= 10000000\n",
    "    will set every house with price greater than 10 mil to 1, \n",
    "    \n",
    "    return 1 if house is predicted to be in SF, 0 if NYC    \n",
    "    '''\n",
    "    ### YOUR CODE BELOW ###\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Test your performance!\n",
    "num_correct = 0\n",
    "sf_but_pred_ny = 0\n",
    "total_num = len(features_c)\n",
    "for i, feature in enumerate(features_c):\n",
    "    prediction_in_sf = is_in_sf(feature)\n",
    "    if prediction_in_sf == labels_c[i]:\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        if labels_c[i] and not prediction_in_sf:\n",
    "            sf_but_pred_ny += 1\n",
    "print(\"Got {:.2f}% correct!\".format(num_correct/1./total_num*100))\n",
    "print(\"Out of incorrect predictions, {:.2f}% were SF houses predicted to be in NYC\".format(\n",
    "                                                                                sf_but_pred_ny/1./(total_num - num_correct)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 The Perceptron Algorithm\n",
    "\n",
    "In this section we will implement the perceptron algorithm, which will learn a linear decision boundary function $f(x)$ of the form:\n",
    "$$\n",
    "f(X_i) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad w^\\top X_i + b > 0 \\\\\n",
    "            0 & \\quad else\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "You can think of this function as drawing a line in the feature space. If a data point is above this line, we'll say it's from SF. If a data point is below this line, we'll say it's from NYC. In the 1D case where there is only 1 feature used, $w$ would be the slope of the line, and $b$ the y-intercept.\n",
    "\n",
    "<br />\n",
    "\n",
    "Note that $f$ is *parameterized* by $w$ and $b$. So our goal is to find the $w$ and $b$ that best minimizes a *loss* function:\n",
    "$$\n",
    "L(X, y) = \\frac{1}{N} \\sum_{i=1}^{N} (f_{w, b}(X_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "The learning update equations for the perceptron algorithm are:\n",
    "$$\n",
    "error = (w^\\top x + b) - y\n",
    "$$\n",
    "$$\n",
    "w = w + \\alpha * error * x\n",
    "$$\n",
    "$$\n",
    "b = b + \\alpha * error\n",
    "$$\n",
    "\n",
    "<br />\n",
    "The perceptron algorithm has 2 *hyperparameters*: the learning rate ($\\alpha$) and the number of epochs to be trained on. In addition, we can also select a smaller set of features instead of using all of them to learn on (sometimes this performs better). Complete the steps below:\n",
    "\n",
    "1. Implement the perceptron algorithm\n",
    "2. Experiment w/ learning rate and epochs. How do these affect the train and test performance? Why?\n",
    "3. Experiment w/ feature selection. Which features seem to work the best? Why?\n",
    "4. Tweak the above settings to get the best test performance. \n",
    "5. If you used 2 or 3 features, you can run the visualization code to visualize the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def f(w, b, x):\n",
    "    if w.dot(x) + b > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def classification_accuracy(X, y, w, b):\n",
    "    y_pred = [f(w, b, x) for x in X]\n",
    "    return (1 - mean_absolute_error(y_pred, y))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Read this implementation. Try matching it with the pseudocode\n",
    "def perceptron_learn_w_b(X_tr, y_tr, X_t, y_t, epochs, learning_rate, features_to_use=None, vis=False):\n",
    "    '''\n",
    "    Run the perceptron algorithm for epochs iterations\n",
    "    Return w, b\n",
    "    '''\n",
    "    # dimensions\n",
    "    N = X_tr.shape[0] # number of data points we have\n",
    "    d = X_tr.shape[1] # dimension of a feature vector\n",
    "    \n",
    "    # initialize weights\n",
    "    w = np.zeros(d) # a vector of 0's of size d\n",
    "    b = 0 # bias starts at 0\n",
    "    \n",
    "    if vis and d in (2, 3):\n",
    "        fig = plt.figure()\n",
    "    \n",
    "    # perceptron learning algorithm\n",
    "    for t in range(epochs):\n",
    "        for i in range(N):            \n",
    "            x = X_tr[i]\n",
    "            pred_y = f(w, b, x)\n",
    "            error = y_tr[i] - pred_y\n",
    "            \n",
    "            ### BEGIN YOUR CODE ###\n",
    "            w =  # Update rule for w\n",
    "            b =  # Update rule for b\n",
    "            ### END YOUR CODE ###\n",
    "        \n",
    "        if vis and d in (2, 3):\n",
    "            fig.clf()\n",
    "            visualize_perceptron(features_to_use, X_tr, y_tr, w, b, fig)\n",
    "            fig.canvas.draw()\n",
    "            sleep(1)\n",
    "        \n",
    "        # reporting accuracy\n",
    "        train_accuracy = classification_accuracy(X_tr, y_tr, w, b)\n",
    "        test_accuracy = classification_accuracy(X_t, y_t, w, b)\n",
    "        #clear_output(wait=True)\n",
    "        print('epoch={}/{} | train={:.2f}% | test={:.2f}%'.format(t+1, epochs, train_accuracy, test_accuracy))\n",
    "                \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Choose hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Choose which features to use\n",
    "# available features are 'bath', 'beds', 'year_built', 'price_per_sqft', 'elevation', 'price'\n",
    "# experiment with a subset of these to find what works best\n",
    "features_to_use_c = ['bath', 'price_per_sqft', 'elevation'] # please use 2 or 3 features here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 4) Run this cell to train perceptron!\n",
    "index_train_split = int(0.8 * len(features_c))\n",
    "features_c_small = extract_cols(features_c, features_to_use_c)\n",
    "features_c_train_array, features_c_test_array = features_c_small[:index_train_split], features_c_small[index_train_split:]\n",
    "labels_c_train_array, labels_c_test_array = np.array(labels_c[:index_train_split]), np.array(labels_c[index_train_split:])\n",
    "\n",
    "w_c, b_c = perceptron_learn_w_b(features_c_train_array, labels_c_train_array, features_c_test_array, labels_c_test_array, \n",
    "                            10, learning_rate, features_to_use=features_to_use_c, vis=True)\n",
    "print('w is ', w_c, 'b is ', b_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) visualize data and learned decision boundary for 2 or 3 features\n",
    "visualize_perceptron(features_to_use_c, features_c_train_array, labels_c_train_array, w_c, b_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Regression on Housing Price\n",
    "\n",
    "In this section, our goal is to learn a model that predicts house prices. Our label is now \"price\" instead of \"in_sf.\" \"in_sf\" is now included as a feature.\n",
    "\n",
    "Regression, unlike classification, predicts a continuous range of values instead of discrete classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1 OLS for Expected House Price\n",
    "\n",
    "In linear regression, we use a linear function to map from input features to output labels. Similar to the perceptron algorithm above, this model has the form:\n",
    "$$\n",
    "y_i = X_i^\\top w\n",
    "$$\n",
    "The new $f$ for regression which outputs the predicted $y$ values will be:\n",
    "$$\n",
    "f(X_i) = X_i^\\top w\n",
    "$$\n",
    "Our loss function will be the same as above:\n",
    "$$\n",
    "L(X, y) = \\frac{1}{N} \\sum_{i=1}^{N} (f_{w}(X_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "We have a closed form solution for this problem. If $X$ is a matrix of features and $y$ the list of labels, then we can write:\n",
    "$$\n",
    "w = (X^\\top X)^{-1} X^\\top y\n",
    "$$\n",
    "\n",
    "To evaluate how good our predictor is, we compute two metrics - mean absolute error and the coefficient of determination (r2 score). Mean absolute error is the average absolute difference of our predicted house price and the true house price. This number however can be difficult to interpret, so we introduce another metric called the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination), or r2 score. R2 scores roughly compute how good a set of predictions are given ground truth data. A higher r2 score means better predictions, and 100% accuracy correspond to an r2 score of 1. \n",
    "\n",
    "Complete the steps below:\n",
    "1. Implement Linear Regression using $b$ as the mean of $y$ and $w$ the pseudoinverse of $X$\n",
    "2. Experiment with which features to use to get the best performance\n",
    "3. Run Linear Regression. Observe results.\n",
    "4. Visualize the linear regression line for when using only 1 feature or 2 features.\n",
    "5. Are the results good or bad? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Implement Linear Regression\n",
    "def linear_regression(X_train, y_train, X_test, y_test):\n",
    "    '''\n",
    "    return weight vector w and bias b\n",
    "    hints:\n",
    "    - np.linalg.pinv(x) returns the (pseudo) inverse of x\n",
    "    - a.dot(b) or a @ b returns the dot product / matrix multiplication of a and b\n",
    "    - x.T returns the transpose of matrix x\n",
    "    '''\n",
    "    ### BEGIN YOUR CODE ###\n",
    "    w =  # Implement closed form solution\n",
    "    ### END YOUR CODE ##\n",
    "    \n",
    "    train_mae = regression_mae(X_train, y_train, w)\n",
    "    test_mae = regression_mae(X_test, y_test, w)\n",
    "    \n",
    "    print(\"Train MAE {} | Test MAE {}\".format(train_mae, test_mae))   \n",
    "    \n",
    "    return w\n",
    "\n",
    "def regression_mae(X, y, w):\n",
    "    y_pred = X.dot(w)\n",
    "    return mean_absolute_error(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Choose which features to use\n",
    "# available features are 'bath', 'beds', 'year_built', 'price_per_sqft', 'elevation', 'in_sf'\n",
    "# experiment with a subset of these to find what works best\n",
    "features_to_use_r = ['elevation']\n",
    "\n",
    "index_train_split = int(0.8 * len(features_r))\n",
    "features_r_small = extract_cols(features_r, features_to_use_r)\n",
    "features_r_biased = np.hstack((features_r_small, np.ones((len(features_r),1))))  # Adding column of ones for bias\n",
    "features_r_train_array, features_r_test_array = features_r_biased[:index_train_split], features_r_biased[index_train_split:]\n",
    "labels_r_train_array, labels_r_test_array = np.array(labels_r[:index_train_split]), np.array(labels_r[index_train_split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Run this cell to run linear regression\n",
    "w_r = linear_regression(features_r_train_array, labels_r_train_array, features_r_test_array, labels_r_test_array)\n",
    "\n",
    "# Computing linear regressor's r2 score using the linear regressor above\n",
    "pred_tr_linear = features_r_train_array.dot(w_r)\n",
    "pred_t_linear = features_r_test_array.dot(w_r)\n",
    "pred_tr_linear_r2 = r2_score(pred_tr_linear, labels_r_train_array)\n",
    "pred_t_linear_r2 = r2_score(pred_t_linear, labels_r_test_array)\n",
    "print(\"Linear Regressor | Train r2 {:.2f}. Test r2 {:.2f}\".format(pred_tr_linear_r2, pred_t_linear_r2))\n",
    "\n",
    "print('w is ', w_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Visualize linear predictor for 1 or 2 features\n",
    "visualize_linear_regression(features_to_use_r, features_r_train_array, labels_r_train_array, w_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 2.2 Regression with Decision Tree\n",
    "\n",
    "The main drawback of linear regression is that it is a model with low expressiveness (or representational power) - it can't fit to complex patterns in data. Another popular method in supervised learning is called Decision Tree. Below we demonstrate fitting this data using decision trees and show the improvements in prediction.\n",
    "\n",
    "<br />\n",
    "Complete the following steps:\n",
    "\n",
    "1. Change the depth hyperparameter and run Decision Tree Regressor\n",
    "\n",
    "2. Compute r2 scores for decision tree\n",
    "\n",
    "3. Slowly increase the depth:\n",
    "    - What depth achieves the best train performance?\n",
    "    - What depth achieves the best test performance?\n",
    "    - At what depth does the test performance begin to decrease? Why?\n",
    "    \n",
    "4. Export and visualize decision tree using dot.\n",
    "    - Which features are being split on? \n",
    "    - Which feature is the most important one?\n",
    "    - Are these feature splits expected/surprising?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Train a Decision Tree Regressor\n",
    "depth = 8 # Experiment with this value to see the balance between train and test performance\n",
    "decision_tree = DecisionTreeRegressor(max_depth=depth)\n",
    "features_dt_train, features_dt_test = features_r_small[:index_train_split], features_r_small[index_train_split:]\n",
    "decision_tree.fit(features_dt_train, labels_r_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2) Compute decision tree regressor's MAE and r2 score\n",
    "pred_tr_decisiontree = decision_tree.predict(features_dt_train)\n",
    "pred_t_decisiontree = decision_tree.predict(features_dt_test)\n",
    "pred_tr_decisiontree_r2 = r2_score(pred_tr_decisiontree, labels_r_train_array)\n",
    "pred_t_decisiontree_r2 = r2_score(pred_t_decisiontree, labels_r_test_array)\n",
    "print(\"Decision Tree Regressor | Train MAE {:.2f}. Test MAE {:.2f}\".format(\n",
    "                            mean_absolute_error(pred_tr_decisiontree, labels_r_train_array),\n",
    "                            mean_absolute_error(pred_t_decisiontree, labels_r_test_array)))\n",
    "print(\"Decision Tree Regressor | Train r2 {:.2f}. Test r2 {:.2f}\".format(pred_tr_decisiontree_r2, pred_t_decisiontree_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) export decision tree\n",
    "export_graphviz(decision_tree, out_file='decision_tree_regressor.dot', feature_names=features_to_use_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, to convert to png format, run command:\n",
    "!dot -Tpng decision_tree_regressor.dot -o outfile.png\n",
    "\n",
    "# Note: This may not work on jupyterhub..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above command did not work, find `decision_tree_regressor.dot` in the Jupyter lab folder and copy paste the contents into contents to this [site](http://dreampuf.github.io/GraphvizOnline/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what you should see:\n",
    "\n",
    "<img src=\"decision_tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning and K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning?\n",
    "\n",
    "What we have done previously is _supervised_ learning. Each feature vector, or observation, in our dataset had a label. Then given some unknown and unlabeled piece of data, use classification or regression to assign it some label.\n",
    "\n",
    "Well, imagine if our dataset had _no_ labels? What could we possibly do with such a dataset that we could call \"machine learning\"? This question has inspired a broad subset of artificial intelligence study called **unsupervised learning**.\n",
    "\n",
    "Unlike supervised learning, here performance is far more subjective. There is no \"ground truth\" that we can use to evaluate the success of our algorithms, rather we are trying to develop an intuition for the underlying structure of our dataset.\n",
    "\n",
    "Here, clustering algorithms are our friend. They allow us to divvy up our datset into a prespecified number of groups to better understand how our observations relate to each other.\n",
    "\n",
    "## Enter K-means\n",
    "\n",
    "The de-facto, Hello World! of clustering algorithms is K-means, an iterative and simple approach to arriving at our k partitions of data (hence the name K-means).\n",
    "\n",
    "At a high level, k-means attempts to find a grouping such that the collective distance between the groups _centroid_, or mean, and all the points that belong in the group is minimized.\n",
    "\n",
    "<img src=\"files/kmeans.png\">\n",
    "\n",
    "1. For each data point, assign it to the closest centroid.\n",
    "2. Calculate the new centroids for each class from the previous assignments.\n",
    "3. Repeat steps 1 and 2 until there is no change to the sum of squared distance between _all_ datapoints and centroids.\n",
    "\n",
    "Now that you have the jist, lets jump into a simpler dataset to see k-means in action!\n",
    "\n",
    "Here we have a small dataset of 3 different types of irises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "\n",
    "# The observations or feature vectors\n",
    "print(data.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names of our features\n",
    "print(data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The different types of irises in our dataset\n",
    "print(data.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can get an idea of (two) of the features, and how the data is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = 0  # Play with which features get displayed to see how the ground truth and K-Means compare\n",
    "feature_2 = 1  # Play with which features get displayed to see how the ground truth and K-Means compare\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(data.data[:, feature_1], data.data[:, feature_2], c=data.target)\n",
    "plt.xlabel(data.feature_names[feature_1])\n",
    "plt.ylabel(data.feature_names[feature_2])\n",
    "plt.title(\"Actual Labels and Features\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here is to use k-means on the unlabeled datset, and see if we can reproduce the classification made by the labels. Let's go ahead and write our k-means algorithm.\n",
    "\n",
    "See if you can improve upon it by adding the stoppping condition from above once we have converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data, num_clusters, max_iter):\n",
    "    \"\"\"Cluster our data into k different groups. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: Array-like matrix with the shape - (n_samples, n_features).\n",
    "    \n",
    "    num_clusters: The number of clusters (and initial centroids) to generate.\n",
    "    \n",
    "    iter: Number of iterations\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    centroids: Array of centroids arrived at during the final iteration.\n",
    "    \n",
    "    labels: Array of shape (n_samples,) that assign data to clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize our centroids with random points. \n",
    "    # More than one way of starting the algorithm! \n",
    "    # See https://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods\n",
    "    centroids = np.random.permutation(data)[:num_clusters]\n",
    "    old_centroids = None\n",
    "    labels = None\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        old_centroids = centroids\n",
    "    \n",
    "        # Assign each data point to closest centroid\n",
    "        labels = find_new_clusters(data, centroids)\n",
    "        \n",
    "        # Find the new centroids given new clusters\n",
    "        centroids = find_new_centroids(data, labels, num_clusters)\n",
    "        \n",
    "        # What conditional could we add here to prematurely halt the algorithm \n",
    "        # and still get the clustering that we want? This will help save time when we have converged.\n",
    "        # HINT: What happens when our new clusters are the same as our old clusters?\n",
    "        # Since we are working with Decimal Numbers, in practice, we can't use np.array_equal since there may be\n",
    "        #  floating-point errors (they are essentially noise, but for decimal numbers)\n",
    "        # Use np.allclose to compare\n",
    "        #### YOUR CODE BELOW ###\n",
    "    \n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_new_clusters(X, centroids):\n",
    "    \"\"\"Assigns data to new clusters that minimizes the distance to the centroids.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: Array-like matrix with the shape - (n_samples, n_features).\n",
    "    \n",
    "    centroids: Array of centroids (k, n_features).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_labels: Array of shape (n_samples,) that assign data to clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    new_labels = np.zeros((X.shape[0],))\n",
    "    \n",
    "    for i, x in enumerate(X):\n",
    "        distances = np.linalg.norm(centroids - x, axis=1)\n",
    "        new_labels[i] = np.argmin(distances)\n",
    "    \n",
    "    return new_labels.astype(int)\n",
    "\n",
    "def find_new_centroids(X, labels, num_clusters):\n",
    "    \"\"\"Finds the new centroids given a set of clusters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: Array-like matrix with the shape - (n_samples, n_features).\n",
    "    \n",
    "    labels: Array of shape (n_samples,) that assign data to clusters.\n",
    "    \n",
    "    num_clusters: Number of clusters\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    new_centroids: Array of centroids (k, n_features).\n",
    "    \"\"\"\n",
    "    new_centroids = np.zeros((num_clusters, X.shape[1]))\n",
    "    \n",
    "    for i in range(num_clusters):\n",
    "        new_centroids[i] = np.mean((X[labels == i]), axis=0)\n",
    "        \n",
    "    return new_centroids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids, labels = kmeans(data.data, 3, 2000) # If this is taking long, see if you can implement the hint in the k-means function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look. How will did this very simple rendition of k-means group our data when you compare it to the actual labeling graphed above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_1 = 0  # Play with which features get displayed to see how the ground truth and K-Means compare\n",
    "feature_2 = 1  # Play with which features get displayed to see how the ground truth and K-Means compare\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.scatter(data.data[:, feature_1], data.data[:, feature_2], c=labels)\n",
    "plt.xlabel(data.feature_names[feature_1])\n",
    "plt.ylabel(data.feature_names[feature_2])\n",
    "plt.title(\"K-Means Clustered Labels and Features\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.scatter(data.data[:, feature_1], data.data[:, feature_2], c=data.target)\n",
    "plt.xlabel(data.feature_names[feature_1])\n",
    "plt.ylabel(data.feature_names[feature_2])\n",
    "plt.title(\"Actual Labels and Features\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the correct color here is not important, because the clustering group is arbitrary. The above clusters _should_ resemble our real labels from above. But sometimes you might not get good classes due to the way we initialized the centroids. Better k-means algorithms will use [better initialization techniques]( https://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods) to enable faster convergence.\n",
    "\n",
    "Go ahead and play with the number of iterations or how many clusters we get. Also try displaying other features to see if K-Means gives a good clustering for all the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets and MNIST\n",
    "We will now explore neural networks using the MNIST dataset.\n",
    "\n",
    "The MNIST dataset is a large collection of handwritten digits commonly used to train and evaluate image processing algorithms. You can see the performance of difference machine learning algorithms here: http://yann.lecun.com/exdb/mnist/ Note that some of the best algorithms can achieve an error rate of less than a percent. Many of those algorithms make use of neural networks, in particular convolutional neural nets (CNNs) which are especially well-suited to image classification.\n",
    "\n",
    "Today our goal is to build a simple neural network to classify these images.\n",
    "<img src=\"files/mnist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a machine learning algorithm well-suited for handling large amounts of data and using it to make predictions or classify objects. Commercial applications of these technologies generally focus on solving complex signal processing or pattern recognition problems. Examples of significant commercial applications since 2000 include handwriting recognition for check processing, speech-to-text transcription, oil-exploration data analysis, weather prediction and facial recognition.\n",
    "\n",
    "A neural network usually involves a large number of neurons operating in parallel and arranged in tiers. The first tier receives the raw input information - analogous to optic nerves in human visual processing. Each successive tier receives the output from the tier preceding it, rather than from the raw input. The last tier produces the output of the system. Each processing node has its own small sphere of knowledge, including what it has seen and any rules it was originally programmed with or developed for itself. In defining the rules and making determinations - that is, each node decides what to send on to the next tier based on its own inputs from the previous tier.\n",
    "\n",
    "Essentially a neural network consists of layers of small units, each with their own weights and responsible for a small amount of computation. Input is fed through these layers and the units work together to produce their decision.\n",
    "\n",
    "(Credit: http://searchnetworking.techtarget.com/definition/neural-network)\n",
    "\n",
    "<img src=\"files/neural_net.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data - Training an Algorithm\n",
    "Good data is the key to a good algorithm. To train a classifier, we need a dataset which tells us the correct output for a given input. Essentially, we need to provide it with examples of what we want it to do (this is called supervised learning). Fortunately for us, scikit-learn allows us to import datasets so we do not have to label thousands of images ourselves.\n",
    "\n",
    "When evaluating a machine learning algorithm it is important to split up the data into train and test sets. This will help us accurately evaluate the effectiveness of our predictions and help prevent overfitting. We use the training set to train our model and then use the test set to see how accurate our model's predictions are. If we trained on the entire dataset, it would be difficult to see how well our algorithm handles new input since we would be giving it data that it has seen before (and thus within our algorithm we have already kind of encoded the answer). The whole point is to make predictions about new data (we already know the answers to the given data) and it would be bad if our algorithm couldn't sufficiently generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sklearn\n",
    "from scipy.io import loadmat\n",
    "mnist = loadmat('mnist-original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data and split it into train and test sets. Take a look at what the images look like and their respective labels (X and Y). Note that the images are stored as an unrolled 1-d vector. We have reconstructed the images by reshaping them to display what they originally looked like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# do some data preprocessing (nothing currently done)\n",
    "processed_data = mnist[\"data\"].T\n",
    "processed_target = mnist[\"label\"][0]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(processed_data, processed_target, test_size=.25)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(X_train[0:5], Y_train[0:5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Digit: %i\\n' % label, fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Classifier\n",
    "\n",
    "<img src=\"files/logistic.png\">\n",
    "\n",
    "Before we make a full neural net, let us look at something a bit simpler - logistic regression. You may have seen a logistic curve in calculus, and we can use it to classify binary data. Logistic regression will allow us to determine the probability of an input being a 0 or a 1. Observe in the graph above that the items towards the left of the curve are more likely to be 0 and the items towards the right of the curve are more likely to be a 1. So if we are given an input value X with an unknown class Y, we can predict its class based on where it lies on the curve.\n",
    "\n",
    "We can use a logistic classifier on its own to predict which digit an image is. We will do this first - imagine that we create one logistic curve for each digit, and for a given input return the digit which has the highest probability.\n",
    "\n",
    "Training a logistic classifier is quite easy with scikit-learn. We will use the L-BFGS algorithm (limited memory Broyden–Fletcher–Goldfarb–Shanno algorithm) as our logistic solver. Note that it might take a bit of time to train. You can read more about the algorithm here: http://aria42.com/blog/2014/12/understanding-lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs', verbose = 10)\n",
    "logisticRegr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our logistic classifier performs. Below you can see a given image, our prediction, and the actual digit. You can play around with the start variable to look at different segments of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 10 # enter a number from 0 to 17495\n",
    "\n",
    "predictions = logisticRegr.predict(X_test[start:start+5])\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, predicted, actual) in enumerate(zip(X_test[start:start+5], predictions, Y_test[start:start+5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Predicted: %i | Actual: %i \\n' %(predicted, actual), fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how it performs on the dataset as a whole. We compare the predicted values with the actual values. You should get an accuracy of around 90%. Decent, but still quite a bit away from today's state-of-the-art algorithms. Let's see if our neural net can perform better. While neural networks can utilize logistic curves / sigmoids, today there are other more commonly used functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logisticRegr.predict(X_test)\n",
    "score = logisticRegr.score(X_test, Y_test)\n",
    "print(\"Logistic classifier accuracy: \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network\n",
    "Before we create our neural network, let us first look at how neural networks make predictions and how they are trained.\n",
    "\n",
    "## The Neurons\n",
    "As mentioned earlier, neural networks are composed of layers of \"neurons.\" Each neuron is responsible for its own little bit of computation. A neuron takes input from the layer before and combines them with some internal weights to generate a single output value. \n",
    "\n",
    "There are many different kinds of layer used in modern neural net architectures.\n",
    "\n",
    "### Fully Connected Layer\n",
    "\n",
    "One of the simplest neurons is called a fully connected (FC) layer, where the input is scaled and shifted to give the output. It is represented by the equation $y = Xw + b$, where where $X$ is the matrix input from past layers and $y$ is the matrix output. Here, the parameters are the weight matrix $w$ and the bias matrix $b$. Before this value is passed onwards to the next layer, however, it must be fed through a nonlinear activation function.\n",
    "We need a nonlinear activation function between every fully connected layer because having two FC layers one after the other is the same as having one FC layer. This is demonstrated in the calculation below. \n",
    "\n",
    "For example: $y_1 = X_1w_1 + b_1$ and $y_2 = X_2w_2 + b_2$. With $y_1$ feeding into $y_2$, the final equation would be: $y_2 = (X_1w_1 + b_1)w_2 + b_2 = X_1(w_1w_2) + (b_1w_2 + b_2)$. \n",
    "\n",
    "Some nonlinear functions include RELU, sigmoid, tanh and maxout. \n",
    "\n",
    "We also get more freedom and control over how the predictions are made, which you can see an visualization for here: https://youtu.be/gmjzbpSVY1A?t=440\n",
    "\n",
    "\n",
    "\n",
    "A single neuron:\n",
    "<img src=\"files/neuron.png\">\n",
    "\n",
    "Input is fed through the layers of neurons in a feedforward neural network.\n",
    "<img src=\"files/neural_net.jpeg\">\n",
    "\n",
    "The RELU activation function $f(x) = max(0, x)$: (note that it's basically just 2 line segments, but still quite powerful)\n",
    "<img src=\"files/relu.jpeg\">\n",
    "\n",
    "### Convolutional Layer\n",
    "\n",
    "Inputs such as images are made up of millions of pixels, with each pixel having 3 values associated with it (R, G, and B). Feeding such a large matrix through an FC Layer would be very computationally expensive. Hence, a new type of layer called a Convolutional Layer was born. Convolutional Layers act as a \"sliding window\", multiplying different sections of the input matrix by the same, smaller weight matrix, called a **kernel**. Such computation is not only cheaper, but can also be paralllelized for maximum efficiency!\n",
    "\n",
    "<img src = \"conv1.png\">\n",
    "<img src = \"conv2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Backpropagation\n",
    "Our neural net will output digit probabilities for each image (a vector containing 10 probabilities). Although it predicts the digit with the highest probability, we still want to consider all the probabilities when training our neural network. For instance our image was a 7, a prediction of 40% for a 7 and 60% for a 9 is better than a prediction of 100% for a 2. We want a metric that will contain that information - not just whether or not we were correct, but how close we were.\n",
    "\n",
    "How do we measure how well our neural network performs? We must define a loss function. The data loss then takes the form of an average over the data losses for every individual example. So, the loss is: (1/N) * (L_1 + ... + L_N). A common loss and the one we'll be using for our neural network today is cross-entropy loss:\n",
    "$$\n",
    "L_i = -\\log\\left(\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\\right)\n",
    "$$\n",
    "This loss is well-suited for prediction problems involving multiple classes (such as digits) and measures the distance between the predicted and actual distribution.\n",
    "\n",
    "# Backpropagation\n",
    "Once we have this loss, how can we use this error to improve our predictions? We want to minimize the loss (minimizing the error) and with neural networks we use a technique called backpropagation. Backpropagation is somewhat similar to gradient descent. In gradient descent, we look at the current slope of the graph and move down the slope towards the \"bottom\" of the hill. You can imagine that with enough iterations, we are able to find a \"valley\" in the graph which represents a minimum. In the graph below observe how the point moves downhill towards the blue valley.\n",
    "<img src=\"gradient_descent.png\">\n",
    "\n",
    "You can imagine that the weights of our neurons are analogous to our coordinates on the graph, the loss as the height, and we look at the error to determine the slope.\n",
    "\n",
    "With backpropagation, we are in essence doing this process for all the neurons in our network. However rather than simply using the final error / loss at the end for all neurons, we instead propagate that error backwards through the network, layer-by-layer. Thus each neuron has its own loss, based on the output it passed on to the layer after it. Backpropagation can thus be thought of as neurons communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to minimize the total loss.\n",
    "\n",
    "The math behind backpropagation relies on the **chain rule** for partial derivatives:\n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial q} \\frac{\\partial q}{\\partial x}$$\n",
    "\n",
    "Say $f(x, y, z)$ is our final output function value and that $f = (2x + 3y)z$.\n",
    "\n",
    "In this case we sort of have 2 layers:\n",
    "* The first is $q = x + y$\n",
    "* The second is $f = q * z$.\n",
    "\n",
    "Let's say we want to see how $x$ affects the final value $f(x, y, z)$. We are given the final value $f(x, y, z)$ along with inputs x, y, z. We can calculate the value of df/dq directly through derivation: $\\frac{df}{dq} = z$. We also know the value of $\\frac{dq}{dx}$ is 2 (since $q = 2x + 3y$). Thus, we see that $\\frac{df}{dx} = 2z$.\n",
    "\n",
    "This is a simple example of backpropagation, but the actual backpropagation for a neural network is essentially the same, just on a larger scale.\n",
    "\n",
    "If you haven't taken multivariable calculus and don't know partial derivatives, this might not make the most sense, but don't worry the library will handle the details of backpropagation for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"chainrule_example.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in how backpropgation works, [here](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html) is an example for backpropogation on a Batchnorm layer (will need some intuition on EECS 16A or Math 54 Linear Algebra)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all Together\n",
    "Let's create our neural net using scikit-learn. Using scikit-learn, we will feed each input in the training set through our neural network. For each input, we will perform backpropagation to improve our prediction and correct errors. Scikit-learn will handle the training for you - all you need to call is clf.fit(X, Y). Then we can check how accurate our neural network is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Change this!!!\n",
    "# The hidden layers in our neural net - each number represents the number \n",
    "# of neurons in a layer (you can add more layers if you desire)\n",
    "hidden_layers = (100, 50, 25)\n",
    "\n",
    "# defines our neural net\n",
    "clf = MLPClassifier(activation = 'relu', learning_rate_init = 1e-3, alpha=1e-5, \n",
    "                    hidden_layer_sizes = hidden_layers, verbose = 10)\n",
    "\n",
    "# trains the neural net\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# evaluates accuracy of neural net\n",
    "score = clf.score(X_test, Y_test)\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"Classifier accuracy: \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around with the classifier parameters in the neural net definition above if you like - MLPClassifier(...), to see if you can improve the neural network. Play around with the number of neurons in each layer (you can also add more layers if you'd like), as the layers are not optimized. When you are done, rerun the code block above and see if you get an improvement. Also take a look at the learning rate and alpha parameter. The highest I could get was around a 96%.\n",
    "\n",
    "For the hidden_layer_sizes parameter, the ith element is the number of neurons in the ith hidden layer. Remember that the hidden layers are the layers of neurons in between the input vector and the final output. Every iteration, the loss should be dropping. If it is not, consider decreasing \"learning_rate_init\" since the gradient might be bouncing back and forth around the valley (lower means that it moves a lesser amount).\n",
    "\n",
    "<img src=\"files/gradient_bouncing.jpeg\">\n",
    "\n",
    "Note that low loss is correlated with predictive power, but you still need to watch out for overfitting (since the network could match the training data very well but not generalize well). Changing the alpha parameter (which controls regularization) will help prevent overfitting - higher alpha means less overfitting.\n",
    "\n",
    "See how high your prediction can get. More neurons and layers may help, but is also slower to train and is more prone to overfitting.\n",
    "\n",
    "\n",
    "The documentation is here: http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can take a look at some of the predictions made by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1087 # enter a number from 0 to 17495\n",
    "\n",
    "predictions = clf.predict(X_test[start:start+5])\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, predicted, actual) in enumerate(zip(X_test[start:start+5], predictions, Y_test[start:start+5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Predicted: %i | Actual: %i \\n' %(predicted, actual), fontsize = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some other algorithms\n",
    "If you have the time, try other machine learning algorithms and see how well they perform. The documentation is here: http://scikit-learn.org/stable/modules/classes. Try playing around with the various parameters (you'll have to look in the documentation to see them).\n",
    "\n",
    "For instance, here's one using a random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# create an instance of the classifier\n",
    "clf = RandomForestClassifier(verbose = 10)\n",
    "\n",
    "# train the classifier (this may take some time)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# print accuracy\n",
    "score = clf.score(X_test, Y_test)\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"Classifier accuracy: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 1097 # enter a number from 0 to 17495\n",
    "\n",
    "predictions = clf.predict(X_test[start:start+5])\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, predicted, actual) in enumerate(zip(X_test[start:start+5], predictions, Y_test[start:start+5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Predicted: %i | Actual: %i \\n' %(predicted, actual), fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Thanks!\n",
    "If you want to learn more about neural networks and machine learning, there are plenty of resources online. Much of this lab was inspired by Stanford's CS231n course here: http://cs231n.github.io/.\n",
    "\n",
    "An example of an online course is this Coursera one on machine learning: https://www.coursera.org/learn/machine-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
