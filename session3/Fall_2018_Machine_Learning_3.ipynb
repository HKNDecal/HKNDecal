{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "#### Going Down the EECS Stack DeCal Fall 2018\n",
    "\n",
    "In this notebook, we'll explore various machine learning techniques and look at a few datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll explore some ideas behind machine learning using housing data from San Francisco and New York. </br>\n",
    "Data courtesy of [r2d3](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "<img src=\"https://grapecollective.com/media/article/image/cache/720x337-center/c/o/comparison.jpg\">\n",
    "Each data point corresponds to a house with the following fields:\n",
    "- year_built (int, e.g. 1990)\n",
    "- price_per_sqft (int, dollars)\n",
    "- bath (float)\n",
    "- beds (float)\n",
    "- elevation (int, ft)\n",
    "- price (int, dollars)\n",
    "- in_sf (int, 1 if in sf, 0 otherwise)\n",
    "\n",
    "In the first part of this notebook we'll try to classify houses and predict whether a house is from NYC or SF. In the second part we'll try to regress and predict the prices of the houses.\n",
    "\n",
    "<br />\n",
    "Mathy Notation for later parts:\n",
    "- $n$ denotes the number of data points (houses) \n",
    "- $d$ denotes the number of features \n",
    "- $X$ is an $n \\times d$ matrix, where each row corresponds to a house. $X_i$ means the $i$th row, or the $i$th feature vector.\n",
    "- $y$ is a length $n$ vector, where each index corresponds to a label. $y_i$ means the label for the $i$th house. For part 1 the labels are either $1$ or $0$ for SF and NYC respectively. For part 2 the labels are real numbers denoting housing price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "%matplotlib notebook\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "from util3 import extract_cols, visualize_linear_regression, visualize_perceptron, load_data\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load housing data\n",
    "all_data, features_c, labels_c, features_r, labels_r = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Classification: NYC vs SF\n",
    "In this section, our goal is to learn a model that predicts whether a given house is from SF or from NYC. Our label is \"is_sf\", which is 1 if the house belongs to SF, 0 if NYC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 Manual Classification\n",
    "1. Use the cells below to explore statistics about the housing data. Note the mean and range of certain features. Which feature helps differentiating nyc and sf houses the most?\n",
    "2. Use your knowledge about the data to fill in the function \"is_in_sf,\" which takes in a feature dictionary and returns 1 if you think this house belongs in SF, 0 if it belongs to NY\n",
    "3. Test how good your result is. Try to improve your score as much as possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_housing_histogram(feature_name):\n",
    "    '''\n",
    "    statistic is either 'mean', 'std', 'min', 'max'\n",
    "    feel free to modify this function to explore other properties about the housing data.\n",
    "    '''\n",
    "    if feature_name not in features_c[0]:\n",
    "        raise ValueError(\"Invalid feature_name!\")\n",
    "        \n",
    "    sf = []\n",
    "    nyc = []\n",
    "    for data in all_data:\n",
    "        if data['in_sf']:\n",
    "            sf.append(data[feature_name])\n",
    "        else:\n",
    "            nyc.append(data[feature_name])\n",
    "    \n",
    "    plt.figure()\n",
    "    bins = np.histogram(np.hstack((sf, nyc)), bins=50)[1]\n",
    "    plt.hist(sf, bins, alpha=0.5, facecolor='red', label='SF')\n",
    "    plt.hist(nyc, bins, alpha=0.5, facecolor='blue', label='NYC')\n",
    "    plt.xlabel(feature_name)\n",
    "    plt.ylabel('number of houses')\n",
    "    plt.title('Histogram of SF and NYC Houses by {}'.format(feature_name))\n",
    "    plt.legend()\n",
    "    \n",
    "    print('SF | mean: {}, std: {}, min: {}, max: {}'.format(np.mean(sf), np.std(sf), min(sf), max(sf)))\n",
    "    print('NYC | mean: {}, std: {}, min: {}, max: {}'.format(np.mean(nyc), np.std(nyc), min(nyc), max(nyc)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) explore the data\n",
    "# available features are price, year_built, bath, beds, elevation, price_per_sqft\n",
    "plot_housing_histogram('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Implement is_in_sf\n",
    "def is_in_sf(feature):\n",
    "    '''\n",
    "    feature is a dictionary with the following keys: \n",
    "    - 'year_built'\n",
    "    - 'price_per_sqft'\n",
    "    - 'bath'\n",
    "    - 'beds'\n",
    "    - 'elevation'\n",
    "    - 'price'\n",
    "    \n",
    "    return 1 if house is predicted to be in SF, 0 if NYC    \n",
    "    '''\n",
    "    ### Your Code Below ###\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Test your performance!\n",
    "num_correct = 0\n",
    "sf_but_pred_ny = 0\n",
    "total_num = len(features_c)\n",
    "for i, feature in enumerate(features_c):\n",
    "    prediction_in_sf = is_in_sf(feature)\n",
    "    if prediction_in_sf == labels_c[i]:\n",
    "        num_correct += 1\n",
    "    else:\n",
    "        if labels_c[i] and not prediction_in_sf:\n",
    "            sf_but_pred_ny += 1\n",
    "print(\"Got {:.2f}% correct!\".format(num_correct/1./total_num*100))\n",
    "print(\"Out of incorrect predictions, {:.2f}% were SF houses predicted to be in NYC\".format(\n",
    "                                                                                sf_but_pred_ny/1./(total_num - num_correct)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 The Perceptron Algorithm\n",
    "\n",
    "In this section we will implement the perceptron algorithm, which will learn a linear decision boundary function $f(x)$ of the form:\n",
    "$$\n",
    "f(X_i) = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad w^\\top X_i + b > 0 \\\\\n",
    "            0 & \\quad else\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "You can think of this function as drawing a line in the feature space. If a data point is above this line, we'll say it's from SF. If a data point is below this line, we'll say it's from NYC. In the 1D case where there is only 1 feature used, $w$ would be the slope of the line, and $b$ the y-intercept.\n",
    "\n",
    "<br />\n",
    "\n",
    "Note that $f$ is *parameterized* by $w$ and $b$. So our goal is to find the $w$ and $b$ that best minimizes a *loss* function:\n",
    "$$\n",
    "L(X, y) = \\frac{1}{N} \\sum_{i=1}^{N} (f_{w, b}(X_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "The learning update equations for the perceptron algorithm are:\n",
    "$$\n",
    "error = (w^\\top x + b) - y\n",
    "$$\n",
    "$$\n",
    "w = w + \\alpha * error * x\n",
    "$$\n",
    "$$\n",
    "b = b + \\alpha * error\n",
    "$$\n",
    "\n",
    "<br />\n",
    "The perceptron algorithm has 2 *hyperparameters*: the learning rate ($\\alpha$) and the number of epochs to be trained on. In addition, we can also select a smaller set of features instead of using all of them to learn on (sometimes this performs better). Complete the steps below:\n",
    "1. Implement the perceptron algorithm\n",
    "2. Experiment w/ learning rate and epochs. How do these affect the train and test performance? Why?\n",
    "3. Experiment w/ feature selection. Which features seem to work the best? Why?\n",
    "4. Tweak the above settings to get the best test performance. \n",
    "5. If you used 2 or 3 features, you can run the visualization code to visualize the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def f(w, b, x):\n",
    "    if w.dot(x) + b > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def classification_accuracy(X, y, w, b):\n",
    "    y_pred = [f(w, b, x) for x in X]\n",
    "    return (1 - mean_absolute_error(y_pred, y))*100\n",
    "\n",
    "def regression_mae(X, y, w, b):\n",
    "    y_pred = X.dot(w) + b\n",
    "    return mean_absolute_error(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Read this implementation. Try matching it with the pseudocode\n",
    "def perceptron_learn_w_b(X_tr, y_tr, X_t, y_t, epochs, learning_rate, features_to_use=None, vis=False):\n",
    "    '''\n",
    "    Run the perceptron algorithm for epochs iterations\n",
    "    Return w, b\n",
    "    '''\n",
    "    # dimensions\n",
    "    N = X_tr.shape[0] # number of data points we have\n",
    "    d = X_tr.shape[1] # dimension of a feature vector\n",
    "    \n",
    "    # initialize weights\n",
    "    w = np.zeros(d) # a vector of 0's of size d\n",
    "    b = 0 # bias starts at 0\n",
    "    \n",
    "    if vis and d in (2, 3):\n",
    "        fig = plt.figure()\n",
    "    \n",
    "    # perceptron learning algorithm\n",
    "    for t in range(epochs):\n",
    "        for i in range(N):            \n",
    "            x = X_tr[i]\n",
    "            pred_y = f(w, b, x)\n",
    "            error = y_tr[i] - pred_y\n",
    "            \n",
    "            ### begin student code ###\n",
    "            \n",
    "            ### end student code ###\n",
    "        \n",
    "        if vis and d in (2, 3):\n",
    "            fig.clf()\n",
    "            visualize_perceptron(features_to_use, X_tr, y_tr, w, b, fig)\n",
    "            fig.canvas.draw()\n",
    "            sleep(1)\n",
    "        \n",
    "        # reporting accuracy\n",
    "        train_accuracy = classification_accuracy(X_tr, y_tr, w, b)\n",
    "        test_accuracy = classification_accuracy(X_t, y_t, w, b)\n",
    "        #clear_output(wait=True)\n",
    "        print('epoch={}/{} | train={:.2f}% | test={:.2f}%'.format(t+1, epochs, train_accuracy, test_accuracy))\n",
    "                \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Choose hyperparameters\n",
    "epochs = 100\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Choose which features to use\n",
    "# available features are 'bath', 'beds', 'year_built', 'price_per_sqft', 'elevation', 'price'\n",
    "# experiment with a subset of these to find what works best\n",
    "features_to_use_c = ['bath', 'beds', 'year_built', 'price_per_sqft', 'elevation'] # please use 2 or 3 features here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 4) Run this cell to train perceptron!\n",
    "index_train_split = int(0.8 * len(features_c))\n",
    "features_c_small = extract_cols(features_c, features_to_use_c)\n",
    "features_c_train_array, features_c_test_array = features_c_small[:index_train_split], features_c_small[index_train_split:]\n",
    "labels_c_train_array, labels_c_test_array = np.array(labels_c[:index_train_split]), np.array(labels_c[index_train_split:])\n",
    "\n",
    "w_c, b_c = perceptron_learn_w_b(features_c_train_array, labels_c_train_array, features_c_test_array, labels_c_test_array, \n",
    "                            10, learning_rate, features_to_use=features_to_use_c, vis=True)\n",
    "print('w is ', w_c, 'b is ', b_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) visualize data and learned decision boundary\n",
    "visualize_perceptron(features_to_use_c, features_c_train_array, labels_c_train_array, w_c, b_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Regression on Housing Price\n",
    "\n",
    "In this section, our goal is to learn a model that predicts house prices. Our label is now \"price\" instead of \"in_sf.\" \"in_sf\" is now included as a feature.\n",
    "\n",
    "Regression, unlike classification, predicts a continuous range of values instead of discrete classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1 OLS for Expected House Price\n",
    "\n",
    "In linear regression, we use a linear function to map from input features to output labels. Similar to the perceptron algorithm above, this model has the form:\n",
    "$$\n",
    "y_i = X_i^\\top w\n",
    "$$\n",
    "The new $f$ for regression which outputs the predicted $y$ values will be:\n",
    "$$\n",
    "f(X_i) = X_i^\\top w\n",
    "$$\n",
    "Our loss function will be the same as above:\n",
    "$$\n",
    "L(X, y) = \\frac{1}{N} \\sum_{i=1}^{N} (f_{w}(X_i) - y_i)^2\n",
    "$$\n",
    "\n",
    "We have a closed form solution for this problem. If $X$ is a matrix of features and $y$ the list of labels, then we can write:\n",
    "$$\n",
    "w = \\inv{(X^\\top X)} X^\\top y\n",
    "$$\n",
    "$$\n",
    "w = X^\\star y\n",
    "$$\n",
    "\n",
    "Where $X^\\star$ is the pseudoinverse of $X$.\n",
    "\n",
    "To evaluate how good our predictor is, we compute two metrics - mean absolute error and the coefficient of determination (r2 score). Mean absolute error is the average absolute difference of our predicted house price and the true house price. This number however can be difficult to interpret, so we introduce another metric called the [coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination), or r2 score. R2 scores roughly compute how good a set of predictions are given ground truth data. A higher r2 score means better predictions, and 100% accuracy correspond to an r2 score of 1. \n",
    "\n",
    "Complete the steps below:\n",
    "1. Implement Linear Regression using $b$ as the mean of $y$ and $w$ the pseudoinverse of $X$\n",
    "2. Experiment with which features to use to get the best performance\n",
    "3. Run Linear Regression. Observe results.\n",
    "4. Visualize the linear regression line for when using only 1 feature or 2 features.\n",
    "5. Are the results good or bad? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Implement Linear Regression\n",
    "def linear_regression(X_tr, y_tr, X_t, y_t):\n",
    "    '''\n",
    "    return weight vector w and bias b\n",
    "    hints:\n",
    "    - np.linalg.pinv returns the pseudoinverse of x\n",
    "    - a.dot(b) returns the dot product of a and b\n",
    "    '''\n",
    "    b = 0\n",
    "    ### begin student code ###\n",
    "    \n",
    "    ### End student code ##\n",
    "    \n",
    "    train_mae = regression_mae(X_tr, y_tr, w, b)\n",
    "    test_mae = regression_mae(X_t, y_t, w, b)\n",
    "    \n",
    "    print(\"Train MAE {} | Test MAE {}\".format(train_mae, test_mae))   \n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Choose which features to use\n",
    "# available features are 'bath', 'beds', 'year_built', 'price_per_sqft', 'elevation', 'in_sf'\n",
    "# experiment with a subset of these to find what works best\n",
    "features_to_use_r = ['bath', 'beds', 'year_built', 'price_per_sqft', 'elevation', 'in_sf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Run this cell to run linear regression\n",
    "index_train_split = int(0.8 * len(features_r))\n",
    "features_r_small = extract_cols(features_r, features_to_use_r)\n",
    "features_r_train_array, features_r_test_array = features_r_small[:index_train_split], features_r_small[index_train_split:]\n",
    "labels_r_train_array, labels_r_test_array = np.array(labels_r[:index_train_split]), np.array(labels_r[index_train_split:])\n",
    "\n",
    "w_r, b_r = linear_regression(features_r_train_array, labels_r_train_array, features_r_test_array, labels_r_test_array)\n",
    "\n",
    "# Computing linear regressor's r2 score using the linear regressor above\n",
    "pred_tr_linear = features_r_train_array.dot(w_r) + b_r\n",
    "pred_t_linear = features_r_test_array.dot(w_r) + b_r\n",
    "pred_tr_linear_r2 = r2_score(pred_tr_linear, labels_r_train_array)\n",
    "pred_t_linear_r2 = r2_score(pred_t_linear, labels_r_test_array)\n",
    "print(\"Linear Regressor | Train r2 {:.2f}. Test r2 {:.2f}\".format(pred_tr_linear_r2, pred_t_linear_r2))\n",
    "\n",
    "print('w is ', w_r, 'b is ', b_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Visualize linear predictor for 1 or 2 features\n",
    "visualize_linear_regression(features_to_use_r, features_r_train_array, labels_r_train_array, w_r, b_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 2.2 Regression with Decision Tree\n",
    "\n",
    "The main drawback of linear regression is that it is a model with low expressiveness (or representational power) - it can't fit to complex patterns in data. Another popular method in supervised learning is called Decision Tree. Below we demonstrate fitting this data using decision trees and show the improvements in prediction.\n",
    "\n",
    "<br />\n",
    "Complete the following steps:\n",
    "1. Change the depth hyperparameter and run Decision Tree Regressor\n",
    "2. Compute r2 scores for decision tree\n",
    "3. Slowly increase the depth from:\n",
    "    - What depth achieves the best train performance?\n",
    "    - What depth achieves the best test performance?\n",
    "    - At what depth does the test performance begin to decrease? Why?\n",
    "4. Export and visualize decision tree using dot. Open .dot file, and copy the contents to this [site](http://dreampuf.github.io/GraphvizOnline/) to generate the visualization. Compare results with your neighbors. \n",
    "    - Which features are being split on? \n",
    "    - Which feature is the most important one?\n",
    "    - Are these feature splits expected/surprising?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Train a Decision Tree Regressor\n",
    "depth = 1 # Experiment with this value to see the balance between train and test performance\n",
    "decision_tree = DecisionTreeRegressor(max_depth=depth)\n",
    "decision_tree.fit(features_r_train_array, labels_r_train_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2) Compute decision tree regressor's MAE and r2 score\n",
    "pred_tr_decisiontree = decision_tree.predict(features_r_train_array)\n",
    "pred_t_decisiontree = decision_tree.predict(features_r_test_array)\n",
    "pred_tr_decisiontree_r2 = r2_score(pred_tr_decisiontree, labels_r_train_array)\n",
    "pred_t_decisiontree_r2 = r2_score(pred_t_decisiontree, labels_r_test_array)\n",
    "print(\"Decision Tree Regressor | Train MAE {:.2f}. Test MAE {:.2f}\".format(\n",
    "                            mean_absolute_error(pred_tr_decisiontree, labels_r_train_array),\n",
    "                            mean_absolute_error(pred_t_decisiontree, labels_r_test_array)))\n",
    "print(\"Decision Tree Regressor | Train r2 {:.2f}. Test r2 {:.2f}\".format(pred_tr_decisiontree_r2, pred_t_decisiontree_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) export decision tree\n",
    "export_graphviz(decision_tree, out_file='decision_tree_regressor.dot', feature_names=features_to_use_r)\n",
    "\n",
    "# view by entering command: dot -Tpng decision_tree_regressor.dot -o outfile.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Nets and MNIST\n",
    "We will now explore neural networks using the MNIST dataset.\n",
    "\n",
    "The MNIST dataset is a large collection of handwritten digits commonly used to train and evaluate image processing algorithms. You can see the performance of difference machine learning algorithms here: http://yann.lecun.com/exdb/mnist/ Note that some of the best algorithms can achieve an error rate of less than a percent. Many of those algorithms make use of neural networks, in particular convolutional neural nets (CNNs) which are especially well-suited to image classification.\n",
    "\n",
    "Today our goal is to build a simple neural network to classify these images.\n",
    "<img src=\"files/mnist.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Neural Network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is a machine learning algorithm well-suited for handling large amounts of data and using it to make predictions or classify objects. Commercial applications of these technologies generally focus on solving complex signal processing or pattern recognition problems. Examples of significant commercial applications since 2000 include handwriting recognition for check processing, speech-to-text transcription, oil-exploration data analysis, weather prediction and facial recognition.\n",
    "\n",
    "A neural network usually involves a large number of neurons operating in parallel and arranged in tiers. The first tier receives the raw input information - analogous to optic nerves in human visual processing. Each successive tier receives the output from the tier preceding it, rather than from the raw input. The last tier produces the output of the system. Each processing node has its own small sphere of knowledge, including what it has seen and any rules it was originally programmed with or developed for itself. In defining the rules and making determinations - that is, each node decides what to send on to the next tier based on its own inputs from the previous tier.\n",
    "\n",
    "Essentially a neural network consists of layers of small units, each with their own weights and responsible for a small amount of computation. Input is fed through these layers and the units work together to produce their decision.\n",
    "\n",
    "(Credit: http://searchnetworking.techtarget.com/definition/neural-network)\n",
    "\n",
    "<img src=\"files/neural_net.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data - Training an Algorithm\n",
    "Good data is the key to a good algorithm. To train a classifier, we need a dataset which tells us the correct output for a given input. Essentially, we need to provide it with examples of what we want it to do (this is called supervised learning). Fortunately for us, scikit-learn allows us to import datasets so we do not have to label thousands of images ourselves.\n",
    "\n",
    "When evaluating a machine learning algorithm it is important to split up the data into train and test sets. This will help us accurately evaluate the effectiveness of our predictions and help prevent overfitting. We use the training set to train our model and then use the test set to see how accurate our model's predictions are. If we trained on the entire dataset, it would be difficult to see how well our algorithm handles new input since we would be giving it data that it has seen before (and thus within our algorithm we have already kind of encoded the answer). The whole point is to make predictions about new data (we already know the answers to the given data) and it would be bad if our algorithm couldn't sufficiently generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sklearn\n",
    "from scipy.io import loadmat\n",
    "mnist = loadmat('mnist-original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data and split it into train and test sets. Take a look at what the images look like and their respective labels (X and Y). Note that the images are stored as an unrolled 1-d vector. We have reconstructed the images by reshaping them to display what they originally looked like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# do some data preprocessing (nothing currently done)\n",
    "processed_data = mnist[\"data\"].T\n",
    "processed_target = mnist[\"label\"][0]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(processed_data, processed_target, test_size=.25)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, label) in enumerate(zip(X_train[0:5], Y_train[0:5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Digit: %i\\n' % label, fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Classifier\n",
    "\n",
    "<img src=\"files/logistic.png\">\n",
    "\n",
    "Before we make a full neural net, let us look at something a bit simpler - logistic regression. You may have seen a logistic curve in calculus, and we can use it to classify binary data. Logistic regression will allow us to determine the probability of an input being a 0 or a 1. Observe in the graph above that the items towards the left of the curve are more likely to be 0 and the items towards the right of the curve are more likely to be a 1. So if we are given an input value X with an unknown class Y, we can predict its class based on where it lies on the curve.\n",
    "\n",
    "We can use a logistic classifier on its own to predict which digit an image is. We will do this first - imagine that we create one logistic curve for each digit, and for a given input return the digit which has the highest probability.\n",
    "\n",
    "Training a logistic classifier is quite easy with scikit-learn. We will use the L-BFGS algorithm (limited memory Broyden–Fletcher–Goldfarb–Shanno algorithm) as our logistic solver. Note that it might take a bit of time to train. You can read more about the algorithm here: http://aria42.com/blog/2014/12/understanding-lbfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logisticRegr = LogisticRegression(solver = 'lbfgs', verbose = 10)\n",
    "logisticRegr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how well our logistic classifier performs. Below you can see a given image, our prediction, and the actual digit. You can play around with the start variable to look at different segments of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0 # enter a number from 0 to 17495\n",
    "\n",
    "predictions = logisticRegr.predict(X_test[start:start+5])\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, predicted, actual) in enumerate(zip(X_test[start:start+5], predictions, Y_test[start:start+5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Predicted: %i | Actual: %i \\n' %(predicted, actual), fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how it performs on the dataset as a whole. We compare the predicted values with the actual values. You should get an accuracy of around 90%. Decent, but still quite a bit away from today's state-of-the-art algorithms. Let's see if our neural net can perform better. While neural networks can utilize logistic curves / sigmoids, today there are other more commonly used functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = logisticRegr.predict(X_test)\n",
    "score = logisticRegr.score(X_test, Y_test)\n",
    "print(\"Logistic classifier accuracy: \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Neural Network\n",
    "Before we create our neural network, let us first look at how neural networks make predictions and how they are trained.\n",
    "\n",
    "## The Neurons\n",
    "As mentioned earlier, neural networks are composed of layers of \"neurons.\" Each neuron is responsible for its own little bit of computation. A neuron takes input from the layer before and combines them with some internal weights to generate a single output value. Before this value is passed onwards to the next layer, it is fed through a nonlinear activation function. Which activation function to use has varied over the years and also depends on the specific implementation, but RELUs (rectified linear units) are quite common today. Some other activation functions include the sigmoid, tanh, and maxout. These nonlinearities are part of what gives neural networks their power and distinguishes them from simple linear regression. With at least one intermediate hidden layer, a neural network is able to represent any continuous function. Hidden layers allow introduce more complexity into our predictor and allow us to classify even nonlinear data.\n",
    "\n",
    "Observe that the inputs are simply combined as a linear combination with their respective weights (along with a bias term) to generate a value. This value is then passed through an activation function for the final output. The RELU activation function is f(x) = max(0, x). These outputs then form the input for the next neurons (depending on the type of layer, a neuron may use some or all of the outputs from the previous layer - we will be using fully connected layers so we will use all outputs from the previous layer). Calculating an output by passing an input through the layers of neurons (each responsible for their own computation) is called feed-forward computation.\n",
    "\n",
    "A single neuron:\n",
    "<img src=\"files/neuron.png\">\n",
    "\n",
    "Input is fed through the layers of neurons in a feedforward neural network.\n",
    "<img src=\"files/neural_net.jpeg\">\n",
    "\n",
    "The RELU activation function f(x) = max(0, x): (note that it's basically just 2 line segments, but still quite powerful)\n",
    "<img src=\"files/relu.jpeg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Backpropagation\n",
    "Our neural net will output digit probabilities for each image (a vector containing 10 probabilities). Although it predicts the digit with the highest probability, we still want to consider all the probabilities when training our neural network. For instance our image was a 7, a prediction of 40% for a 7 and 60% for a 9 is better than a prediction of 100% for a 2. We want a metric that will contain that information - not just whether or not we were correct, but how close we were.\n",
    "\n",
    "How do we measure how well our neural network performs? We must define a loss function. The data loss then takes the form of an average over the data losses for every individual example. So, the loss is: (1/N) * (L_1 + ... + L_N). A common loss and the one we'll be using for our neural network today is cross-entropy loss:\n",
    "<img src=\"files/cross_entropy.png\">\n",
    "This loss is well-suited for prediction problems involving multiple classes (such as digits) and measures the distance between the predicted and actual distribution.\n",
    "\n",
    "# Backpropagation\n",
    "Once we have this loss, how can we use this error to improve our predictions? We want to minimize the loss (minimizing the error) and with neural networks we use a technique called backpropagation. Backpropagation is somewhat similar to gradient descent. In gradient descent, we look at the current slope of the graph and move down the slope towards the \"bottom\" of the hill. You can imagine that with enough iterations, we are able to find a \"valley\" in the graph which represents a minimum. In the graph below observe how the point moves downhill towards the blue valley.\n",
    "<img src=\"files/gradient_descent.png\">\n",
    "\n",
    "You can imagine that the weights of our neurons are analogous to our coordinates on the graph, the loss as the height, and we look at the error to determine the slope.\n",
    "\n",
    "With backpropagation, we are in essence doing this process for all the neurons in our network. However rather than simply using the final error / loss at the end for all neurons, we instead propagate that error backwards through the network, layer-by-layer. Thus each neuron has its own loss, based on the output it passed on to the layer after it. Backpropagation can thus be thought of as neurons communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to minimize the total loss.\n",
    "\n",
    "The math behind backpropagation relies on the chain rule for partial derivatives:\n",
    "<img src=\"files/chain_rule.png\">\n",
    "Say f(x, y, z) is our final output function value and that f = (2x + 3y)z. In this case we sort of have 2 layers - the first is q = x + y and the second is f = qz. Let's say we want to see how x affects the final value f(x, y, z). We are given the final value f(x, y, z) along with inputs x, y, z. We can calculate the value of df/dq directly through derivation: df/dq = z. We also know the value of dq/dx is 2 (since q = 2x + 3y). Thus, we see that df/dx = 2z.\n",
    "\n",
    "This is a simple example of backpropagation, but the actual backpropagation for a neural network is essentially the same, just on a larger scale.\n",
    "\n",
    "If you haven't taken multivariable calculus and don't know partial derivatives, this might not make the most sense, but don't worry the library will handle the details of backpropagation for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all Together\n",
    "Let's create our neural net using scikit-learn. Using scikit-learn, we will feed each input in the training set through our neural network. For each input, we will perform backpropagation to improve our prediction and correct errors. Scikit-learn will handle the training for you - all you need to call is clf.fit(X, Y). Then we can check how accurate our neural network is.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Change this!!!\n",
    "# The hidden layers in our neural net - each number represents the number \n",
    "# of neurons in a layer (you can add more layers if you desire)\n",
    "hidden_layers = (100, 50)\n",
    "\n",
    "# defines our neural net\n",
    "clf = MLPClassifier(activation = 'relu', learning_rate_init = 1e-3, alpha=1e-5, \n",
    "                    hidden_layer_sizes = hidden_layers, verbose = 10)\n",
    "\n",
    "# trains the neural net\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# evaluates accuracy of neural net\n",
    "score = clf.score(X_test, Y_test)\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"Classifier accuracy: \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play around with the classifier parameters in the neural net definition above if you like - MLPClassifier(...), to see if you can improve the neural network. Play around with the number of neurons in each layer (you can also add more layers if you'd like), as the layers are not optimized. When you are done, rerun the code block above and see if you get an improvement. Also take a look at the learning rate and alpha parameter. The highest I could get was around a 96%.\n",
    "\n",
    "For the hidden_layer_sizes parameter, the ith element is the number of neurons in the ith hidden layer. Remember that the hidden layers are the layers of neurons in between the input vector and the final output. Every iteration, the loss should be dropping. If it is not, consider decreasing \"learning_rate_init\" since the gradient might be bouncing back and forth around the valley (lower means that it moves a lesser amount).\n",
    "\n",
    "<img src=\"files/gradient_bouncing.jpeg\">\n",
    "\n",
    "Note that low loss is correlated with predictive power, but you still need to watch out for overfitting (since the network could match the training data very well but not generalize well). Changing the alpha parameter (which controls regularization) will help prevent overfitting - higher alpha means less overfitting.\n",
    "\n",
    "See how high your prediction can get. More neurons and layers may help, but is also slower to train and is more prone to overfitting.\n",
    "\n",
    "\n",
    "The documentation is here: http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can take a look at some of the predictions made by the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0 # enter a number from 0 to 17495\n",
    "\n",
    "predictions = clf.predict(X_test[start:start+5])\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, predicted, actual) in enumerate(zip(X_test[start:start+5], predictions, Y_test[start:start+5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Predicted: %i | Actual: %i \\n' %(predicted, actual), fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try some other algorithms\n",
    "If you have the time, try other machine learning algorithms and see how well they perform. The documentation is here: http://scikit-learn.org/stable/modules/classes. Try playing around with the various parameters (you'll have to look in the documentation to see them).\n",
    "\n",
    "For instance, here's one using a random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# create an instance of the classifier\n",
    "clf = RandomForestClassifier(verbose = 10)\n",
    "\n",
    "# train the classifier (this may take some time)\n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "# print accuracy\n",
    "score = clf.score(X_test, Y_test)\n",
    "print(\"~~~~~~~~~~~~~~~~~~~~~\")\n",
    "print(\"Classifier accuracy: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0 # enter a number from 0 to 17495\n",
    "\n",
    "predictions = clf.predict(X_test[start:start+5])\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "for index, (image, predicted, actual) in enumerate(zip(X_test[start:start+5], predictions, Y_test[start:start+5])):\n",
    "    plt.subplot(1, 5, index + 1)\n",
    "    plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n",
    "    plt.title('Predicted: %i | Actual: %i \\n' %(predicted, actual), fontsize = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thanks!\n",
    "If you want to learn more about neural networks and machine learning, there are plenty of resources online. Much of this lab was inspired by Stanford's CS231n course here: http://cs231n.github.io/.\n",
    "\n",
    "An example of an online course is this Coursera one on machine learning: https://www.coursera.org/learn/machine-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
